{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkAZi0QCQRc8QIxqsgK/hg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cinalimaster/ariftanis/blob/main/Text%20Summarization/GPT-3%20Text%20Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PNMHLxWOake"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s100FO0Odz5"
      },
      "source": [
        "Steps to summarize a paper with GPT-3\n",
        "The process itself is quite simple:\n",
        "1.\tDownload the paper\n",
        "2.\tConvert from pdf to text\n",
        "3.\tFeed the text to the GPT-3 model using the openai api\n",
        "4.\tShow the summary\n",
        "1. Download the paper\n",
        "First let’s import our dependencies\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=xTwy6RBkfr0\n",
        "\n",
        "https://medium.com/geekculture/a-paper-summarizer-with-python-and-gpt-3-2c718bc3bc88"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsPzLjShOgBP"
      },
      "source": [
        "import openai\n",
        "import wget\n",
        "import pathlib\n",
        "import pdfplumber\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE5Ma7_9Ohfo"
      },
      "source": [
        "Here you will have to install openai for interfacing with GPT-3 in case you have an api key, if you don’t have it you can join the wait list here.\n",
        "\n",
        "You will also need wget for downloading the pdf from the arxiv page and pdfplumberfor converting the pdf to text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyUPb3fROrbk"
      },
      "source": [
        "pip install openai\n",
        "pip install wget\n",
        "pip install pdfplumber\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA-wuWOvOs_E"
      },
      "source": [
        "Now, let’s write a function that downloads a pdf from an arxiv address, the paper I will be using is ‘Understanding training and generalization in deep learning by Fourier analysis’ by Zhi-Qin John Xu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUAqVvcAOttk"
      },
      "source": [
        "def getPaper(paper_url, filename=\"random_paper.pdf\"):\n",
        "    \"\"\"\n",
        "    Downloads a paper from it's arxiv page and returns\n",
        "    the local path to that file.\n",
        "    \"\"\"\n",
        "    downloadedPaper = wget.download(paper_url, filename)    \n",
        "    downloadedPaperFilePath = pathlib.Path(downloadedPaper)\n",
        "\n",
        "    return downloadedPaperFilePath\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8bMFpmYOwDk"
      },
      "source": [
        "Here, I am using the wget package to directly download the pdf and return a path to the downloaded file.\n",
        "2. Convert from pdf to text\n",
        "Now, I will write another function to convert the pdf to text so that GPT-3 can actually read it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed3injrQO0OT"
      },
      "source": [
        "paperFilePath = \"random_paper.pdf\"\n",
        "paperContent = pdfplumber.open(paperFilePath).pages\n",
        "\n",
        "def displayPaperContent(paperContent, page_start=0, page_end=5):\n",
        "    for page in paperContent[page_start:page_end]:\n",
        "        print(page.extract_text())\n",
        "displayPaperContent(paperContent)# Output \n",
        "Understanding training and generalization in deeplearning by Fourier analysisZhi-QinJohnXu∗8NewYorkUniversityAbuDhabi1AbuDhabi129188,UnitedArabEmirates0zhiqinxu@nyu.edu2 voN Abstract 92 Background: It is still an open research area to theoretically understand why  DeepNeuralNetworks(DNNs)—equippedwithmanymoreparametersthantrain- ] ing data and trained by (stochastic) gradient-based methods—often achieve re-Gmarkably low generalization error. Contribution: We study DNN training byL Fourier analysis. Our theoretical frameworkexplains: i) DNN with (stochastic). gradient-based methods often endows low-frequency components of the targetsc function with a higher priority during the training; ii) Small initialization leads[ to good generalizationability of DNN while preservingthe DNN’s ability to ﬁt   any function. These results are further conﬁrmed by experiments of DNNs\n",
        "...\n",
        "...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVQhRe8tO16Y"
      },
      "source": [
        "Here, I extracted the text per page from the paper and wrote a function displayPaperContent to show the corresponding content. There are issues with the conversion that I won’t address in this article because I want to focus just on the summarization pipeline.\n",
        "3. Feed the text to the GPT-3 model using the openai api\n",
        "Now, for the fun stuff, let’s write a function that get’s the paper content and feeds it to the GPT-3 model using openai’s api:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhKTLgSyO3Lg"
      },
      "source": [
        "def showPaperSummary(paperContent):\n",
        "    tldr_tag = \"\\n tl;dr:\"\n",
        "    openai.organization = 'organization key'\n",
        "    openai.api_key = \"your api key\"\n",
        "    engine_list = openai.Engine.list() # calling the engines available from the openai api \n",
        "    \n",
        "    for page in paperContent:    \n",
        "        text = page.extract_text() + tldr_tag\n",
        "        response = openai.Completion.create(engine=\"davinci\",prompt=text,temperature=0.3,\n",
        "            max_tokens=140,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "        print(response[\"choices\"][0][\"text\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl-QD745O464"
      },
      "source": [
        "Let’s unpack what is happening here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSMPFw6XO7fY"
      },
      "source": [
        "tldr_tag = \"\\n tl;dr:\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcqjBnsmO8wY"
      },
      "source": [
        "In this step I am writing the tag so that the GPT-3 model knows when the text stops and when it should start the completion (which in this case is a summary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzqpsVz4O-bA"
      },
      "source": [
        "openai.organization = \"the openai organization key\"\n",
        "openai.api_key = \"your api key\"\n",
        "engine_list = openai.Engine.list() # calling the engines available  # from the openai api\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqiTtS8GO_RY"
      },
      "source": [
        "Here, I am setting up the environment for using the openai API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk-_OZ0WPAX4"
      },
      "source": [
        "for page in paperContent:    \n",
        "        text = page.extract_text() + tldr_tag\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_F0d5q0PCfQ"
      },
      "source": [
        "Now, I am extracting the text from each page and feeding it to the GPT-3 model. In the future I want to write an extension to this script so that it can get paragraph by paragraph, so that the model could see semantically connected chunks from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MkPiamtPDmw"
      },
      "source": [
        "response = openai.Completion.create(engine=\"davinci\",prompt=text,temperature=0.3,\n",
        "            max_tokens=140,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "        print(response[\"choices\"][0][\"text\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRcQVjoPE-s"
      },
      "source": [
        "Finally, I am feeding the text to the model setting a max tokens of 140 for the response (half the size of a tweet) for each page and printing that to the terminal.\n",
        "4. Show the summary\n",
        "Ok great! Now that we have our model set up, let’s run it and see the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0uqtBJRPHLt"
      },
      "source": [
        "paperContent = pdfplumber.open(paperFilePath).pages\n",
        "showPaperSummary(paperContent)# Output:The power spectrum of the tanh function exponentially decays w.r.t. frequency.\n",
        "\n",
        "Thegradient-basedtrainingwiththeDNNwithonehiddenlayerwithtanhfunction\n",
        "ThetotallossfunctionofasinglehiddenlayerDNNis\n",
        "\n",
        "\n",
        " The initial weights of a deep neural network are more important than the initial biases.\n",
        " We can use Fourier analysis to understand the gradient-based optimization of DNNs on real data and pure noise. We found that DNNs have a better generalization ability when the loss function is ﬂatter. We also found that the DNN generalization ability is better when the DNN parameters are smaller.\n",
        "\n",
        " The code is available at https://github.com/yuexin-wang/DeepLearning-Music .\n",
        "\n",
        " Theorem 3 is true.\n",
        " We can use the same method to prove the theorem.\n",
        " The DNN is a very powerful tool that can be used for many things. It is not a panacea, but it is a very powerful tool.\n",
        " Deep learning is only one of many tools for building intelligent systems.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYWG6abWPITU"
      },
      "source": [
        "And there it is! This is so cool! Due to issues with the pdf conversion, two page summaries appeared glued together but overall it gives an ok description of the paper page by page, even highlighting the source code link!\n",
        "Putting it all together\n",
        "The full code is this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQwDtYfEPKsE"
      },
      "source": [
        "import openai\n",
        "import wget\n",
        "import pathlib\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "\n",
        "def getPaper(paper_url, filename=\"random_paper.pdf\"):\n",
        "    \"\"\"\n",
        "    Downloads a paper from it's arxiv page and returns\n",
        "    the local path to that file.\n",
        "    \"\"\"\n",
        "    downloadedPaper = wget.download(paper_url, filename)    \n",
        "    downloadedPaperFilePath = pathlib.Path(downloadedPaper)\n",
        "\n",
        "    return downloadedPaperFilePath\n",
        "\n",
        "\n",
        "def showPaperSummary(paperContent):\n",
        "    tldr_tag = \"\\n tl;dr:\"\n",
        "    openai.organization = 'API KEY org'\n",
        "    openai.api_key = \"your openAI key\"\n",
        "    engine_list = openai.Engine.list() \n",
        "    \n",
        "    for page in paperContent:    \n",
        "        text = page.extract_text() + tldr_tag\n",
        "        response = openai.Completion.create(engine=\"davinci\",prompt=text,temperature=0.3,\n",
        "            max_tokens=140,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "        print(response[\"choices\"][0][\"text\"])\n",
        "\n",
        "\n",
        "paperContent = pdfplumber.open(paperFilePath).pages\n",
        "showPaperSummary(paperContent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXbflX5PLck"
      },
      "source": [
        "**Thoughts on Summarization**\n",
        "\n",
        "I think summarization models are great. They will never replace the important process of actually reading an entire paper, but they can serve as a tool to explore a wider range of interesting scientific discoveries.\n",
        "This post was more a proof of concept, mostly because the issues with formatting the text converted from the pdf and the actual quality of the summarization are still things to tweak quite a bit. However, I feel like we have come a long way and now, more than ever, we are getting closer to having something that will actually allow us to process a bigger array of scientific information.\n"
      ]
    }
  ]
}