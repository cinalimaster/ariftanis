{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "\n",
    "def kaggle_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred));\n",
    "#def kaggle_score(y_true,y_pred):\n",
    "#    return np.sqrt(mean_squared_error(np.log(y_true), np.log(y_pred)));\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train = pd.read_csv(\"../datasets/house_prices/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "Run DecisionTreeRegressor on House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 28521.569254185695\n",
      "Test MAPE: 15.568382942404535\n",
      "Test Kaggle: 0.21400016507169195\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = DecisionTreeRegressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor\n",
    "Run sklearn's GradientBoostingRegressor on House Prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 16451.61231456698\n",
      "Test MAPE: 9.25024550760628\n",
      "Test Kaggle: 0.1293182192154727\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = GradientBoostingRegressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor from scratch\n",
    "Let us write GradientBoostingRegressor from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators = 10, shrinkage = 0.1):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.shrinkage = shrinkage\n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.shrinkage * model.predict(X)\n",
    "        grads = y - preds\n",
    "        return grads\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.shrinkage * m.predict(X)\n",
    "        return preds\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=2);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor from scratch\n",
    "Now let us run our version of GradientBoostingRegressor on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 16634.622608431117\n",
      "Test MAPE: 9.683086170975127\n",
      "Test Kaggle: 0.1301372397573366\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = MyGradientBoostingRegressor(n_estimators=100, shrinkage=0.1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm (Gradient Descent in the Function Space)\n",
    "\n",
    "Gradient boosting makes gradient descent on the prediction values. If we use L2 loss then the loss function becomes:\n",
    "\n",
    "$$ \\sum_{i=1}^n(y_i -\\hat{y}_i)^2$$ \n",
    "\n",
    "Note that in the above equation we know the actual $y$ values. Suppose that we start at some random fixed value (say 0) for all $\\hat{y}_i$'s. Be careful to note that here we are treating $\\hat{y}_i$'s as variables. In order to minimize the above loss function which direction should we go for each $\\hat{y}_i$. In other words, should we decrease or increase the values of $\\hat{y}_i$'s. We know that we can answer this question by looking at the gradient. That is, the update rule should be like this:\n",
    "\n",
    "$$ \\hat{y}_i = \\hat{y}_i - \\alpha \\frac{\\partial \\sum_{i=1}^n(y_i -\\hat{y}_i)^2 }{\\partial \\hat{y}_i }$$ \n",
    "which is equal to\n",
    "$$ \\hat{y}_i = \\hat{y}_i + \\alpha(y_i -\\hat{y}_i)$$ \n",
    "\n",
    "where $\\alpha$ is the learning rate (or shrinkage). The somewhat counter intuitive thing we will do now is to learn the amount of update (that is, $ \\alpha(y_i -\\hat{y}_i)$ with a base learner.\n",
    "\n",
    "This treatment is a bit confusing. For further reading, [here](https://explained.ai/gradient-boosting/) is a nice exposition of gradient boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class LeastSquares:\n",
    "    @staticmethod\n",
    "    def gradient(y, p):\n",
    "        return -(y - p)\n",
    "    def loss(self, y, p):\n",
    "        return 0.5 * np.power((y - p), 2)\n",
    "\n",
    "\n",
    "class My2GradientBoostingRegressor:\n",
    "\n",
    "    def __init__(self, shrinkage=0.1, loss=LeastSquares(), n_estimators=100):\n",
    "        self.shrinkage = shrinkage\n",
    "        self.loss = loss\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        if (len(self.models)>0):\n",
    "            return -sum(m.predict(X) for m in self.models)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for m in range(self.n_estimators):\n",
    "            preds = self.predict(X)\n",
    "            gradients = self.loss.gradient(y, preds)\n",
    "            tree = DecisionTreeRegressor(max_depth=2)\n",
    "            tree.fit(X, self.shrinkage*gradients)\n",
    "            self.models.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.989232929640268\n",
      "8.470903278093923\n",
      "8.642362455248106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg2 = GradientBoostingRegressor()\n",
    "reg3 = My2GradientBoostingRegressor(shrinkage=0.2)\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "reg3.fit(X_train, y_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_test, reg1.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg2.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bank Marketing Dataset from\n",
    "# https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "bank = pd.read_csv(\"../datasets/bank/bank-full.csv\", delimiter = \";\")\n",
    "# print first 5 examples\n",
    "bank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     39922\n",
       "yes     5289\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    5289\n",
       "no     5289\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_majority = bank[bank.y==\"no\"]\n",
    "bank_minority = bank[bank.y==\"yes\"]\n",
    " \n",
    "# downsample\n",
    "bank_majority_downsampled = resample(bank_majority, \n",
    "                                 replace=False,    \n",
    "                                 n_samples=5289) \n",
    " \n",
    "bank_balanced = pd.concat([bank_minority, bank_majority_downsampled])\n",
    "bank_balanced.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossEntropy:\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        #return y - p\n",
    "        return (y / p) - (1 - y) / (1 - p)\n",
    "\n",
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, shrinkage=0.1, loss=CrossEntropy(), n_estimators=100):\n",
    "        self.shrinkage = shrinkage\n",
    "        self.loss = loss\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []\n",
    "        \n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "    def cutoff(self, x):\n",
    "        if (x > 0.5):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self.cutoff(y) for y in self.predict_raw(X)]\n",
    "        \n",
    "    def predict_raw(self, X):\n",
    "        if (len(self.models)>0):\n",
    "            return self.sigmoid(sum(m.predict(X) for m in self.models))\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        for m in range(self.n_estimators):\n",
    "            preds = self.predict_raw(X)\n",
    "            #print(preds)\n",
    "            gradients = self.loss.gradient(y, preds)\n",
    "            #print(gradients)\n",
    "            tree = DecisionTreeRegressor(max_depth=2)\n",
    "            tree.fit(X, self.shrinkage*gradients)\n",
    "            self.models.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       542\n",
      "           1       0.79      0.78      0.79       516\n",
      "\n",
      "    accuracy                           0.79      1058\n",
      "   macro avg       0.79      0.79      0.79      1058\n",
      "weighted avg       0.79      0.79      0.79      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       542\n",
      "           1       0.85      0.88      0.86       516\n",
      "\n",
      "    accuracy                           0.86      1058\n",
      "   macro avg       0.86      0.86      0.86      1058\n",
      "weighted avg       0.86      0.86      0.86      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84       542\n",
      "           1       0.82      0.84      0.83       516\n",
      "\n",
      "    accuracy                           0.83      1058\n",
      "   macro avg       0.83      0.83      0.83      1058\n",
      "weighted avg       0.83      0.83      0.83      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.14965159573065\n",
      "9.265885655738185\n",
      "9.937858315046913\n",
      "8.516827668811349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg2 = GradientBoostingRegressor()\n",
    "reg3 = My2GradientBoostingRegressor()\n",
    "reg4 = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "reg3.fit(X_train, y_train)\n",
    "reg4.fit(X_train, y_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_test, reg1.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg2.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg3.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost can handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.231988732557712\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "reg4 = xgb.XGBRegressor()\n",
    "reg4.fit(X_train, y_train)\n",
    "print(mean_absolute_percentage_error(y_test, reg4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tevfikaytekin/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:21:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       538\n",
      "           1       0.78      0.80      0.79       520\n",
      "\n",
      "    accuracy                           0.79      1058\n",
      "   macro avg       0.79      0.79      0.79      1058\n",
      "weighted avg       0.79      0.79      0.79      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.84       538\n",
      "           1       0.81      0.88      0.84       520\n",
      "\n",
      "    accuracy                           0.84      1058\n",
      "   macro avg       0.84      0.84      0.84      1058\n",
      "weighted avg       0.84      0.84      0.84      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       538\n",
      "           1       0.78      0.78      0.78       520\n",
      "\n",
      "    accuracy                           0.79      1058\n",
      "   macro avg       0.79      0.79      0.79      1058\n",
      "weighted avg       0.79      0.79      0.79      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       538\n",
      "           1       0.83      0.88      0.86       520\n",
      "\n",
      "    accuracy                           0.85      1058\n",
      "   macro avg       0.86      0.85      0.85      1058\n",
      "weighted avg       0.86      0.85      0.85      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "clf4 = xgb.XGBClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "clf4.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test)\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:21:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tevfikaytekin/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87       522\n",
      "           1       0.86      0.90      0.88       536\n",
      "\n",
      "    accuracy                           0.87      1058\n",
      "   macro avg       0.87      0.87      0.87      1058\n",
      "weighted avg       0.87      0.87      0.87      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "clf4 = xgb.XGBClassifier()\n",
    "clf4.fit(X_train, y_train);\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
