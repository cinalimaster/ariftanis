{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8nmvzS1wal2"
   },
   "source": [
    "# Word2Vec from Scratch\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtGrCtTzwal3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import gutenberg, brown\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from queue import PriorityQueue\n",
    "\n",
    "\n",
    "# You need to call nltk.download() to download all the nltk corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Lok_93Ewal6"
   },
   "source": [
    "## Definition from Wikipedia:\n",
    "\n",
    "“Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "slHGBPk9wal6",
    "outputId": "201100d6-fa43-4572-bdf0-5ffb7595c1f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/tevfikaytekin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 57340\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "num_sents = len(brown.sents())\n",
    "print(\"number of sentences:\", num_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UNiOp5Xwal-"
   },
   "source": [
    "An example sentence represented as a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "hTeu-Agwwal_",
    "outputId": "21cafd26-d787-4237-8609-faf1eb800c2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is an example application of word2vec using Gensim library. You can see some of the parameters and can find all the details of Gensim implementation [here](https://radimrehurek.com/gensim/models/word2vec.html). The Gensim word2vec source code is [here](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py) and the original source code by Mikolov can be found [here](https://github.com/tmikolov/word2vec/blob/master/word2vec.c).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "ShLkVpzUwamF",
    "outputId": "e1dc9dcb-d178-43d8-8160-063126edbfe9"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(brown.sents(),min_count = 5,\n",
    "                              size = 30, window = 5, iter=5, negative=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCLJktzTwamI"
   },
   "source": [
    "An example vector representation of the word \"book\". Since we set size = 30, the representation is an array of 30 reals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "hzmfOyduwamJ",
    "outputId": "2fe7d901-63e3-49db-bb37-7d882e3104f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26579976, -0.04866867, -1.7459654 , -0.6115054 , -0.5542139 ,\n",
       "        0.8283093 ,  0.41846105, -0.04969763, -0.8726043 , -0.07534609,\n",
       "       -0.37399644,  0.34195694, -0.07170147,  0.50888443,  0.52798414,\n",
       "       -0.55636686,  0.7330005 ,  0.02543076,  0.7189497 ,  0.6839749 ,\n",
       "       -0.49252346,  0.45718732,  0.54439783,  0.12184591,  0.2821921 ,\n",
       "       -0.32804507, -1.1608193 ,  0.7788689 , -0.4628607 , -0.26199132],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to test the performance of word2vec is to look at most similar words to a given word. Below you will find most similar words of the words \"book\" and \"eight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "lf2Nt7qhwamL",
    "outputId": "5bab91d5-2ed6-4e70-e17c-a3a03e6f815f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('story', 0.9440566897392273),\n",
       " ('opinion', 0.9329782724380493),\n",
       " ('artist', 0.9126719236373901),\n",
       " ('novel', 0.9116226434707642),\n",
       " ('gapt', 0.9088972806930542),\n",
       " ('hero', 0.9082541465759277),\n",
       " ('statement', 0.9060463309288025),\n",
       " ('era', 0.9050542712211609),\n",
       " ('poem', 0.902087390422821),\n",
       " ('name', 0.9006975889205933)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "fAeXOOtpwamN",
    "outputId": "2126b29e-c02a-4a74-f898-c9ba852cc8ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seven', 0.9629417061805725),\n",
       " ('thirty', 0.943234920501709),\n",
       " ('fifteen', 0.9374372959136963),\n",
       " ('nine', 0.9368208050727844),\n",
       " ('65', 0.9360512495040894),\n",
       " ('eleven', 0.9357398152351379),\n",
       " ('flights', 0.9344480037689209),\n",
       " ('decades', 0.9337546825408936),\n",
       " ('twelve', 0.9332236051559448),\n",
       " ('previous', 0.9326519966125488)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='eight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the results are quite amazing. But it might not be so for every word, for example for the word \"angry\" the results are not very satisying. However, if we have used a larger text the results could be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('through', 0.851377546787262),\n",
       " ('into', 0.8492832183837891),\n",
       " ('over', 0.8413790464401245),\n",
       " ('toward', 0.8371174335479736),\n",
       " ('along', 0.8329604864120483),\n",
       " ('from', 0.8304653167724609),\n",
       " ('behind', 0.8277620077133179),\n",
       " ('against', 0.8031615614891052),\n",
       " ('across', 0.7904695272445679),\n",
       " ('around', 0.781238317489624)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "Nw3uZuNdxAxO",
    "outputId": "2d6acd8a-7942-4f2b-d860-6dbccc218417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('verses', 0.9653607606887817),\n",
       " ('pathetic', 0.9590141177177429),\n",
       " ('dressed', 0.9582985639572144),\n",
       " ('guilt', 0.9556961059570312),\n",
       " ('impartial', 0.9556828737258911),\n",
       " ('imagery', 0.9555773735046387),\n",
       " ('master', 0.9553168416023254),\n",
       " ('complement', 0.9551525115966797),\n",
       " ('ankle', 0.9546823501586914),\n",
       " ('minister', 0.9534153938293457)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='angry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlOxTodOwamP"
   },
   "source": [
    "You can also find (cosine) similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6nNBVelwamQ",
    "outputId": "75c3a85c-ac2b-448f-ad53-0c99cfb53fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'book' and 'story': 0.9440565\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'book' and 'story':\", \n",
    "    model.wv.similarity('book', 'story')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'book' and 'eight': 0.5177929\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'book' and 'eight':\", \n",
    "    model.wv.similarity('book', 'eight')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEs8zFMOwamS"
   },
   "source": [
    "## word2vec from scratch\n",
    "\n",
    "Now we will write word2vec from scratch. Note that the purpose of this implementation is to help understand the theory behind word2vec. The implementation is not meant to be efficient so the running time is quite slow compared to the Gensim implementation. However, the code is simpler and shows the main ingredients of word2vec.\n",
    "\n",
    "Different objectives can be used for word2vec. The following is the objective for word2vec with negative sampling. The main idea behind this objective is to find paramater values which maximizes the dot product of word representations which are in the same context and minimizes the dot product of word representations which are not in the same context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKPGXjrHwamS"
   },
   "source": [
    "$$\n",
    "J(\\Theta) = \\underset{\\theta}{\\operatorname{argmax}}{\\sum_{c,t \\in D_p}log(\\sigma(v_c \\cdot v_t))+\\sum_{c,t \\in D_n}log(\\sigma(-v_c \\cdot v_t))}\n",
    "$$\n",
    "\n",
    "Here, $D_p$ is the set of word pairs whose distance is at most $m$ and $D_n$ is the set of unrelated (negative) word pairs, i.e., word pairs whose distance is larger than $m$, and $\\sigma$ is the sigmoid function. Below we find the derivative of this function with respect to positive and negative words which we will use in the updates of gradient descent algorithm.\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial v_{c}}=\\sum_{c,t \\in D_p}\\frac{1}{\\sigma(v_c \\cdot v_t)}\\sigma(v_c \\cdot v_t)(1-\\sigma(v_c \\cdot v_t))(v_t)\\\\\n",
    "+ \\sum_{c,t \\in D_n}\\frac{1}{\\sigma(-v_c \\cdot v_t)}\\sigma(-v_c \\cdot v_t)(1-\\sigma(-v_c \\cdot v_t))(-v_t)\\\\\n",
    "= \\sum_{c,t \\in D_p}(1-\\sigma(v_c \\cdot v_t))v_t + \\sum_{c,t \\in D_n}-(1-\\sigma(-v_c \\cdot v_t))v_t \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial v_{t \\in D_p}}=\\sum_{c,t \\in D_p}(1-\\sigma(v_c \\cdot v_t))v_c \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial v_{t \\in D_n}}=\\sum_{c,t \\in D_n}-(1-\\sigma(-v_c \\cdot v_t))v_c \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmjwFkqPwamT"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Efrh96rzwama"
   },
   "outputs": [],
   "source": [
    "def build_indices(sents):\n",
    "    \"\"\" \n",
    "  \n",
    "    Parameters: \n",
    "    sents: A list of sentecens and each sentence is a list of words (i.e., a list of lists). \n",
    "  \n",
    "    Returns: \n",
    "    word_freqs: frequency of each word\n",
    "    word_to_index: a mapping from word names to integers.\n",
    "    index_to_word: a mapping from integers to word names.\n",
    "    \n",
    "  \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    word_freqs = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    for i in range(len(sents)): \n",
    "        for j in range(len(sents[i])):\n",
    "            w = sents[i][j].lower()\n",
    "            if w in word_freqs:\n",
    "                word_freqs[w] += 1\n",
    "            else:\n",
    "                word_freqs[w] = 1\n",
    "                word_to_index[w] = counter\n",
    "                index_to_word[counter] = w\n",
    "                counter += 1\n",
    "            \n",
    "    return word_freqs, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICcBKeI2wame"
   },
   "outputs": [],
   "source": [
    "def build_training_set(sents, word_freqs, window=5, sampling_freq = 0.001, neg_exp = 0.75, num_negs = 1, min_count=5):\n",
    "    \"\"\" \n",
    "    Builds a trainig set\n",
    "    \n",
    "    Parameters: \n",
    "    sents: A list of sentecens and each sentence is a list of words (i.e., a list of lists).\n",
    "    word_freqs: Frequency of words.\n",
    "    windows: size of the context window.\n",
    "    sampling_freq: words whose frequency larger than this value will be discarded.\n",
    "    neg_exp: used for adjusting the negative sampling distribution.\n",
    "    min_count: \n",
    "  \n",
    "    Returns: \n",
    "    training_set: list of context word, positive and negatives\n",
    "    \"\"\"\n",
    "    words_list = []\n",
    "    total_freq = sum(word_freqs.values())\n",
    "    \n",
    "    #total_freq = sum([freq**(neg_exp) for freq in word_freqs.values()])\n",
    "    word_array = []\n",
    "    for word, freq in word_freqs.items():\n",
    "        if ((word_freqs[word]/total_freq) < sampling_freq) and (word_freqs[word] > min_count):\n",
    "            words_list.append(word)\n",
    "            for i in range(int(freq**neg_exp)):\n",
    "                word_array.append(word)\n",
    "    \n",
    "    training_set = []\n",
    "    \n",
    "    sampled_sents = []\n",
    "    for i in range(len(sents)): \n",
    "        sent = []\n",
    "        for j in range(len(sents[i])):\n",
    "            w = sents[i][j].lower()\n",
    "            if ((word_freqs[w] / total_freq) < sampling_freq) and (word_freqs[w] > min_count):\n",
    "                sent.append(w)\n",
    "        sampled_sents.append(sent)\n",
    "    \n",
    "    \n",
    "    for i in range(len(sampled_sents)): \n",
    "        for j, w in enumerate(sampled_sents[i]):\n",
    "            context = []\n",
    "            for k in range(max(j-window,0),min(j+window+1,len(sampled_sents[i]))):\n",
    "                w_p = sampled_sents[i][k]\n",
    "                if (w == w_p):\n",
    "                    continue\n",
    "                w_n = []\n",
    "                for k in range(num_negs):\n",
    "                    w_n.append(word_array[np.random.randint(0,len(word_array))] )\n",
    "                training_set.append([w,w_p,w_n])\n",
    "\n",
    "    return training_set, np.unique(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the produced training_set here is a very simple example sentence consisting of 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ha5PR9xJwamg",
    "outputId": "2e87c77c-02b3-4a49-a844-826abad9b79e"
   },
   "outputs": [],
   "source": [
    "sents = [[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"]]\n",
    "word_freqs = {\"a\":1,\"b\":1,\"c\":1,\"d\":1,\"e\":1,\"f\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "XdzNepMvwamj",
    "outputId": "62979a2d-e445-4590-a6db-4de64b966a19"
   },
   "outputs": [],
   "source": [
    "training_set, words_list = build_training_set(sents,window=1, word_freqs= word_freqs , sampling_freq = 1, min_count= 0, num_negs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b', ['c', 'e']],\n",
       " ['b', 'a', ['b', 'e']],\n",
       " ['b', 'c', ['e', 'a']],\n",
       " ['c', 'b', ['a', 'e']],\n",
       " ['c', 'd', ['d', 'b']],\n",
       " ['d', 'c', ['c', 'f']],\n",
       " ['d', 'e', ['c', 'a']],\n",
       " ['e', 'd', ['f', 'b']],\n",
       " ['e', 'f', ['f', 'c']],\n",
       " ['f', 'e', ['c', 'b']]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print training set\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now build the training set for the brown dataset which will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs, word_to_index, index_to_word = build_indices(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "uh8TDnKxwaml",
    "outputId": "a2fe935e-117d-441d-f8cc-d8a3f548b2ae"
   },
   "outputs": [],
   "source": [
    "training_set, words_list = build_training_set(brown.sents(),word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "yJD0RYekHXBD",
    "outputId": "3858e3b7-05bd-4c84-e11a-08a6deac335c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fulton', 'county', ['separate']],\n",
       " ['fulton', 'grand', ['hanover']],\n",
       " ['fulton', 'jury', ['korean']],\n",
       " ['fulton', 'friday', ['critical']],\n",
       " ['fulton', 'investigation', ['miniature']],\n",
       " ['county', 'fulton', ['measures']],\n",
       " ['county', 'grand', ['l.']],\n",
       " ['county', 'jury', ['taxable']],\n",
       " ['county', 'friday', ['mine']],\n",
       " ['county', 'investigation', ['physically']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 10 examples in the trainigng set\n",
    "training_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3255198"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "28etYfROwamx",
    "outputId": "76579b98-d211-4af2-86b5-4916a188e536"
   },
   "outputs": [],
   "source": [
    "def build_model(training_set, initial_alpha = 0.025, min_alpha = 0.0001, n_iters = 5, my_lambda = 0, vector_size = 30):\n",
    "    word_vectors = {}\n",
    "    \n",
    "    # initialize word vectors\n",
    "    for n in range(len(words_list)):\n",
    "        word_vectors[words_list[n]] = np.random.rand(vector_size,1) - 0.5\n",
    "    \n",
    "\n",
    "    alpha = initial_alpha\n",
    "    for t in range(n_iters):\n",
    "        training_set = shuffle(training_set)\n",
    "        objective = 0\n",
    "        print(\"cosine of words 'friend' and 'fellow': \",np.dot(word_vectors['friend'].T, word_vectors['fellow']))\n",
    "        for ex in training_set:\n",
    "            w = ex[0]\n",
    "            w_p = ex[1]\n",
    "            w_n_list = ex[2]\n",
    "            w_v = word_vectors[w]\n",
    "            w_p_v = word_vectors[w_p]\n",
    "            word_vectors[w_p] = w_p_v + alpha*(((1-sigmoid(np.dot(w_v.T,w_p_v)))*w_v)-my_lambda*w_p_v)\n",
    "            objective += np.log((sigmoid(np.dot(w_v.T,w_p_v))))\n",
    "\n",
    "            for n in range(len(w_n_list)):\n",
    "                w_n = w_n_list[n]\n",
    "                w_n_v = word_vectors[w_n]\n",
    "                word_vectors[w_n] = w_n_v + alpha*((-(1-sigmoid(-np.dot(w_v.T,w_n_v)))*w_v)-my_lambda*w_n_v)      \n",
    "                objective += np.log((sigmoid(-np.dot(w_v.T,w_p_v))))\n",
    "     \n",
    "        alpha = initial_alpha - ((initial_alpha - min_alpha) * t / n_iters)\n",
    "        print(\"alpha: \",alpha)\n",
    "        print(\"Iteration: \", t)\n",
    "        print(\"Objective: \", objective)\n",
    "    print(\"cosine of words 'friend' and 'fellow': \",np.dot(word_vectors['friend'].T, word_vectors['fellow']))\n",
    "\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine of words 'friend' and 'fellow':  [[-0.28009892]]\n",
      "alpha:  0.025\n",
      "Iteration:  0\n",
      "Objective:  [[-4629741.23324028]]\n",
      "cosine of words 'friend' and 'fellow':  [[-0.17475462]]\n",
      "alpha:  0.020020000000000003\n",
      "Iteration:  1\n",
      "Objective:  [[-4762469.56499669]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.37146372]]\n",
      "alpha:  0.015040000000000001\n",
      "Iteration:  2\n",
      "Objective:  [[-4901301.54166369]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.63088071]]\n",
      "alpha:  0.010060000000000001\n",
      "Iteration:  3\n",
      "Objective:  [[-4987241.78843278]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.89783896]]\n",
      "alpha:  0.005080000000000001\n",
      "Iteration:  4\n",
      "Objective:  [[-5038862.9510509]]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = build_model(training_set, initial_alpha = 0.025, min_alpha = 0.0001, n_iters = 5, my_lambda = 0, vector_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine of words 'friend' and 'fellow':  [[0.95898103]]\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine of words 'friend' and 'fellow': \",np.dot(word_vectors['friend'].T, word_vectors['fellow']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4a8La9LNwamz"
   },
   "outputs": [],
   "source": [
    "def most_similar(word, word_vectors):\n",
    "    pq = PriorityQueue()\n",
    "    for w in word_vectors.keys():\n",
    "        pq.put((-np.dot(word_vectors[word].T, word_vectors[w]), w))\n",
    "    return pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi0EKiQwwam1"
   },
   "outputs": [],
   "source": [
    "pq = most_similar('book', word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "7ndrjm5wwam2",
    "outputId": "9ae23791-b143-4ca2-a15d-bb908ff0d374",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-3.16413106]]), 'book')\n",
      "(array([[-2.37720672]]), 'james')\n",
      "(array([[-2.28934352]]), 'david')\n",
      "(array([[-2.08540662]]), 'rayburn')\n",
      "(array([[-2.06554837]]), 'wrote')\n",
      "(array([[-2.03319124]]), 'poet')\n",
      "(array([[-2.03146274]]), 'poems')\n",
      "(array([[-2.02708295]]), 'athletics')\n",
      "(array([[-1.96002009]]), 'history')\n",
      "(array([[-1.9494699]]), 'soul')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(pq.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "meIMEyIPwam4",
    "outputId": "9987faa7-42ca-4dad-cab3-290edf8a9a77",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-3.63661662]]), 'eight')\n",
      "(array([[-2.94130014]]), 'hundred')\n",
      "(array([[-2.77708548]]), '31')\n",
      "(array([[-2.76095622]]), 'year')\n",
      "(array([[-2.62741732]]), 'nine')\n",
      "(array([[-2.52538252]]), '20')\n",
      "(array([[-2.45696699]]), 'cent')\n",
      "(array([[-2.36731842]]), 'thousand')\n",
      "(array([[-2.33627123]]), 'four')\n",
      "(array([[-2.32009323]]), 'years')\n"
     ]
    }
   ],
   "source": [
    "pq = most_similar('eight', word_vectors)\n",
    "for i in range(10):\n",
    "    print(pq.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Even though \"opinion\" and \"story\" does not appear in the context of \"book\" (window size 5) still word2vec puts these close to book, which is remarkable. Can you find this by building a co-occurrence matrix?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
