{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, brown\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Matrix\n",
    "table taken from [https://wordnetcode.princeton.edu/5papers.pdf](https://wordnetcode.princeton.edu/5papers.pdf)\n",
    "\n",
    "<img src=\"images/lexical_matrix.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "The word meaning $M_1$ in above table can be represented by the set of word forms that can be used to express it: {F1, F2, . . . }. These sets are called synonym sets (or simply synsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet Hierarchy\n",
    "Below is a simplified illustration of the hierarchy of wordnet.\n",
    "\n",
    "<img src=\"images/wordnet_hierarchy.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word is a set of meanings\n",
    "wn.synsets('word') gives these meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synset is a set of synonyms (word forms). Each synset corresponds to a concept/meaning. The nodes in the WordNet hierarchy corresponds to synsets. A synset is identified with a 3-part name of the form: word.pos.nn. For example, 'car.n.01' means the first meaning of 'car' used as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('car.n.01').definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas correspond to word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'railcar', 'railway_car', 'railroad_car']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.02').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypernyms anf hyponyms\n",
    "\n",
    "In linguistics, hyponymy means a subtype and a hypernym means a supertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonymy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Similarity\n",
    "path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.16666666666666666\n",
      "0.07692307692307693\n",
      "0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "print(right.path_similarity(minke))\n",
    "print(right.path_similarity(orca))\n",
    "print(right.path_similarity(tortoise))\n",
    "print(right.path_similarity(novel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wordnet_hierarchy.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "compact = wn.synset('compact.n.03')\n",
    "hatchback = wn.synset('hatchback.n.01')\n",
    "print(motorcar.path_similarity(compact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(hatchback.path_similarity(compact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(hatchback.path_similarity(hatchback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ways for finding synonyms\n",
    "\n",
    "WordNet is constructed manually by experts of linguistics. There is also the computational approach to semantics. Below we will look at one such approach for finding synonyms. The approach relies on the below fundamental hypothesis:\n",
    "<br><br>\n",
    "<center><b>Distributional Hypothesis: similar words appear in similar contexts.</b></center>\n",
    "<br><br>\n",
    "We will first need to build a corpus and a co-occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(text):\n",
    "    \"\"\" \n",
    "  \n",
    "    Parameters: \n",
    "    text (string): A (long) string \n",
    "  \n",
    "    Returns: \n",
    "    words: A list of unique word names.\n",
    "    word_to_index: a mapping from word names to integers.\n",
    "    index_to_word: a mapping from integers to word names.\n",
    "  \n",
    "    \"\"\"\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [] \n",
    "    for i in sent_tokenize(text): \n",
    "        for j in tokenizer.tokenize(i):\n",
    "            words.append(j.lower())\n",
    "    words = np.unique(words)\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words = [porter.stem(t) for t in words]\n",
    "    words = np.unique(words)\n",
    "    \n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    counter = 0;\n",
    "    for w in words:\n",
    "        word_to_index[w] = counter\n",
    "        index_to_word[counter] = w\n",
    "        counter += 1  \n",
    "    return words, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is data mining course cmp5101. It is about data mining. I like it so much.\"\n",
    "words, word_to_index, index_to_word = build_corpus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about' 'cmp5101' 'cours' 'data' 'i' 'is' 'it' 'like' 'mine' 'much' 'so'\n",
      " 'thi']\n",
      "{'about': 0, 'cmp5101': 1, 'cours': 2, 'data': 3, 'i': 4, 'is': 5, 'it': 6, 'like': 7, 'mine': 8, 'much': 9, 'so': 10, 'thi': 11}\n"
     ]
    }
   ],
   "source": [
    "corpus_size = len(words)\n",
    "print(words)\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27114\n",
      "number of tokens:  2084675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['235', '236', '237', '239', '23a', '23d', '23rd', '24', '240',\n",
       "       '2400', '242', '2433', '2454', '247', '24th', '25', '250', '2500',\n",
       "       '251', '253', '254', '2544', '255', '256', '257', '258', '25th',\n",
       "       '26', '260', '261', '265', '266', '268', '2688', '269', '26th',\n",
       "       '27', '270', '2705', '271', '272', '273', '2731', '275', '276',\n",
       "       '278', '27th', '28', '280', '2809', '281', '2825', '283', '285',\n",
       "       '286', '28th', '29', '293', '294', '295', '297', '2991', '29th',\n",
       "       '2a', '2d', '2nd', '3', '30', '300', '3000', '300th', '302', '306',\n",
       "       '307', '30th', '31', '310', '3100', '312', '313', '314', '31730',\n",
       "       '3181', '31978', '31st', '32', '320', '320tr', '3211', '323',\n",
       "       '3247', '325', '327', '328', '33', '330', '3300', '332', '334',\n",
       "       '337', '338', '33d', '34', '340', '340tr', '34220', '343', '344',\n",
       "       '346', '348', '35', '350', '3500', '3505o', '350th', '353', '354',\n",
       "       '355', '357', '36', '360', '361', '362', '3646', '365', '366',\n",
       "       '367', '368', '369', '36th', '37', '372', '375', '376', '379',\n",
       "       '37th', '38', '380', '381', '385', '387', '389', '39', '390',\n",
       "       '391', '392', '394', '395', '399', '3m', '3mm', '3rd', '4', '40',\n",
       "       '400', '4000', '401', '402', '405', '407', '41', '410', '412',\n",
       "       '413', '414', '415', '417', '418', '419', '42', '420', '4200',\n",
       "       '425', '427', '42d', '43', '430', '431', '434', '44', '44001',\n",
       "       '44002', '44005', '44006', '44007', '441', '442', '443', '444',\n",
       "       '447', '45', '450', '451', '452', '457', '46', '460', '461', '462',\n",
       "       '463', '469', '46th', '47', '470', '48', '480', '484', '4865771',\n",
       "       '488', '489', '49', '490', '4911', '492', '495', '499', '49er',\n",
       "       '49th', '4th', '5', '50', '500', '5000', '503', '5031', '505',\n",
       "       '508', '509', '50th', '51', '510', '511', '512', '514', '514c',\n",
       "       '5155', '517', '51st', '52', '520', '524', '525', '526', '52h',\n",
       "       '52nd', '53', '532', '538', '54', '540', '541', '542', '543',\n",
       "       '545', '54th', '55', '550', '553', '555', '557', '56', '560',\n",
       "       '561', '5612', '562', '565', '566', '56a', '57', '570', '571',\n",
       "       '573', '5777', '58', '580', '581', '5835', '5847', '585', '586',\n",
       "       '589', '58th', '59', '590', '5th', '6', '60', '600', '6000', '601',\n",
       "       '602', '603', '604', '605', '606', '607', '608', '609', '61',\n",
       "       '610', '6124', '613', '615', '616', '617', '619', '61st', '62',\n",
       "       '622', '625', '63', '634', '635', '637', '638', '639', '63d', '64',\n",
       "       '642', '643', '645', '646', '65', '650', '66', '666', '66th', '67',\n",
       "       '671', '675', '676', '677', '679', '68', '687', '689', '69',\n",
       "       '6934', '694', '695', '6a', '6th', '7', '70', '700', '701',\n",
       "       '701st', '7026', '7034', '704', '707', '7070', '7074', '70th',\n",
       "       '71', '710', '72', '720', '725', '7287', '72nd', '73', '734',\n",
       "       '738', '74', '740', '741', '742', '742c', '744', '748', '75',\n",
       "       '750', '753', '754', '758', '7599', '75th', '76', '760', '762',\n",
       "       '764', '767', '768', '77', '770', '78', '780', '78th', '79', '790',\n",
       "       '792', '795', '798', '7a', '7th', '8', '80', '800', '807', '80th',\n",
       "       '81', '816', '817', '82', '821', '823', '825', '827', '828', '83',\n",
       "       '833', '836', '83rd', '84', '840', '841', '842', '85', '850', '86',\n",
       "       '865', '867', '869', '87', '870', '871', '877', '87th', '88',\n",
       "       '883', '885', '8861', '887', '89', '892', '899', '8th', '9', '90',\n",
       "       '900', '901', '906', '91', '910', '918', '92', '920', '923',\n",
       "       '9230', '926', '93', '9329', '938', '94', '940i', '944', '949',\n",
       "       '95', '950', '954', '96', '960', '961', '963', '97', '98', '987',\n",
       "       '989', '99', '991', '9a', '9b', '9e', '9n', '9th', 'a', 'a135',\n",
       "       'a40', 'a5', 'aa', 'aaa', 'aaawww', 'aab', 'aah', 'aaron', 'ab',\n",
       "       'ab1', 'ab4', 'ab63711', 'aback', 'abandon', 'abaring', 'abas',\n",
       "       'abat', 'abatuno', 'abb', 'abba', 'abber', 'abbey', 'abbot',\n",
       "       'abbott', 'abbrevi', 'abc', 'abdallah', 'abdomen', 'abdomin',\n",
       "       'abdomini', 'abduct', 'abe', 'abel', 'abelson', 'aber',\n",
       "       'abernathi', 'aberr', 'abet', 'abey', 'abhor', 'abhorr', 'abid',\n",
       "       'abigail', 'abil', 'abilen', 'abject', 'abjectli', 'abl', 'ablard',\n",
       "       'ablat', 'ablaz', 'abler', 'abli', 'abn', 'abner', 'abnorm', 'abo',\n",
       "       'aboard', 'abod', 'abolish', 'abolit', 'abolitionist', 'aborigin',\n",
       "       'abort', 'abound', 'about', 'abov', 'aboveground', 'abra',\n",
       "       'abraham', 'abram', 'abras', 'abreact', 'abreast', 'abridg',\n",
       "       'abroad', 'abrog', 'abrupt', 'abruptli', 'abscess', 'abscissa',\n",
       "       'absenc', 'absent', 'absente', 'absentia', 'absentmindedli',\n",
       "       'absinth', 'absolut', 'absorb', 'absorpt', 'abstain', 'abstent',\n",
       "       'abstin', 'abstract', 'abstracted', 'abstraction',\n",
       "       'abstractionist', 'abstractli', 'abstractor', 'abstrus', 'absurd',\n",
       "       'absurdli', 'abund', 'abundantli', 'abus', 'abut', 'abx', 'abysm',\n",
       "       'abyss', 'abyssinian', 'ac', 'acacia', 'academ', 'academeh',\n",
       "       'academi', 'academicianship', 'acadia', 'acala', 'acapulco',\n",
       "       'accacia', 'accademia', 'accardo', 'acced', 'acceler',\n",
       "       'acceleromet', 'accent', 'accentu', 'accept', 'access',\n",
       "       'accessori', 'accid', 'accident', 'acclaim', 'acclam', 'acclimat',\n",
       "       'accolad', 'accommod', 'accomod', 'accompani', 'accompanist',\n",
       "       'accomplic', 'accomplish', 'accord', 'accordingli', 'accordion',\n",
       "       'accost', 'account', 'accouter', 'accredit', 'accret', 'accru',\n",
       "       'accultur', 'accumul', 'accur', 'accuraci', 'accus', 'accusingli',\n",
       "       'accustom', 'ace', 'acet', 'aceton', 'acetonemia', 'acey', 'ach',\n",
       "       'achaean', 'acheson', 'achiev', 'achil', 'acid', 'acidul',\n",
       "       'ackerli', 'acknowledg', 'acolyt', 'acont', 'acorn', 'acours',\n",
       "       'acoust', 'acquaint', 'acquies', 'acquiesc', 'acquir', 'acquisit',\n",
       "       'acquit', 'acquitt', 'acr', 'acreag', 'acrid', 'acrobaci',\n",
       "       'acrobat', 'acropoli', 'across', 'acryl', 'act', 'acth',\n",
       "       'actinomet', 'action', 'activ', 'activit', 'actor', 'actress',\n",
       "       'actual', 'actuari', 'actuat', 'acumen', 'acut', 'ad', 'ada',\n",
       "       'adag', 'adagio', 'adair', 'adam', 'adamantli', 'adamo', 'adamson',\n",
       "       'adapt', 'adaptaplex', 'adc', 'adcock', 'add', 'addabbo', 'addict',\n",
       "       'addison', 'addit', 'addl', 'address', 'addresse', 'adduc', 'ade',\n",
       "       'adel', 'adelia', 'adelo', 'adenau', 'adenoma', 'adept', 'adequ',\n",
       "       'adequaci', 'aderhold', 'adher', 'adhes', 'adieu', 'adio', 'adip',\n",
       "       'adirondack', 'adjac', 'adject', 'adjectiv', 'adjoin', 'adjourn',\n",
       "       'adjud', 'adjudg', 'adjunct', 'adjust', 'adlai', 'adler',\n",
       "       'admassi', 'administ', 'administr', 'adminstr', 'admir',\n",
       "       'admiralti', 'admiringli', 'admiss', 'admit', 'admitt',\n",
       "       'admittedli', 'admix', 'admonish', 'admonit', 'adnan', 'ado',\n",
       "       'adob', 'adolesc', 'adolf', 'adolphu', 'adoni', 'adoniram',\n",
       "       'adopt', 'ador', 'adorn', 'adrar', 'adren', 'adrian', 'adrianopl',\n",
       "       'adriat', 'adrien', 'adrift', 'adroit', 'adsorb', 'adul', 'adult',\n",
       "       'adulter', 'adulteri', 'adulthood', 'advanc', 'advantag', 'advent',\n",
       "       'adventist', 'adventiti', 'adventur', 'adverb', 'adverbi',\n",
       "       'advers', 'adversari', 'advertis', 'advic', 'advis', 'advisedli',\n",
       "       'advisor', 'advisori', 'advoc', 'advocaci', 'aec', 'aegean',\n",
       "       'aegi', 'aeon', 'aerat', 'aerial', 'aerob', 'aerobact',\n",
       "       'aerodynam', 'aerogen', 'aeronaut', 'aerosol', 'aerospac',\n",
       "       'aeschbach', 'aeschylu', 'aesthet', 'aeternitati', 'af', 'afar',\n",
       "       'affabl', 'affair', 'affect', 'affectingli', 'affection', 'affer',\n",
       "       'affi', 'affianc', 'affidavit', 'affili', 'affin', 'affirm',\n",
       "       'affix', 'afflict', 'affluenc', 'affluent', 'afford', 'affront',\n",
       "       'afghan', 'aficionado', 'afield', 'afir', 'afl', 'aflam', 'afloat',\n",
       "       'afo', 'afoot', 'aforement', 'aforesaid', 'aforethought', 'afraid',\n",
       "       'afranio', 'afresh', 'africa', 'african', 'afrika', 'afriqu',\n",
       "       'afro', 'aft', 'after', 'aftermath', 'afternoon', 'afterward',\n",
       "       'aftuh', 'again', 'against', 'agamemnon', 'agat', 'agatha',\n",
       "       'agayn', 'age', 'ageless', 'agenc', 'agenda', 'agent', 'ager',\n",
       "       'aggi', 'agglomer', 'agglutin', 'agglutinin', 'aggrandis',\n",
       "       'aggrandiz', 'aggrav', 'aggreg', 'aggress', 'aggressor', 'aggriev',\n",
       "       'aghast', 'agil', 'agin', 'agit', 'agleam', 'agn', 'agnes',\n",
       "       'agnomen', 'agnost', 'ago', 'agoeng', 'agon', 'agonal', 'agoni',\n",
       "       'agrarian', 'agre', 'agreeabl', 'agreement', 'agricola',\n",
       "       'agricultur', 'agrippa', 'agrobacterium', 'agu', 'ah', 'ah6',\n",
       "       'ahah', 'ahai', 'ahead', 'ahem', 'ahm', 'ahmad', 'ahmet', 'ahmiri',\n",
       "       'ahn', 'ahren', 'ai', 'aia', 'aich', 'aid', 'aida', 'aiee',\n",
       "       'aiken', 'aikin', 'ail', 'aileron', 'ailey', 'ailment', 'aim',\n",
       "       'aimless', 'aimlessli', 'aimo', 'ain', 'ainsley', 'ainsworth',\n",
       "       'ainu', 'air', 'airborn', 'aircraft', 'airdrop', 'airedal',\n",
       "       'airfield', 'airflow', 'airfram', 'airi', 'airili', 'airless',\n",
       "       'airlift', 'airlin', 'airlock', 'airmail', 'airman', 'airmen',\n",
       "       'airpark', 'airplan', 'airport', 'airspe', 'airstrip', 'airway',\n",
       "       'aisl', 'aj', 'aja', 'ajar', 'ajb', 'ajk', 'akin', 'akita',\n",
       "       'akron', 'aku', 'al', 'ala', 'alabama', 'alabaman', 'alabamian',\n",
       "       'alabast', 'alacr', 'alai', 'alain', 'alamein', 'alamo',\n",
       "       'alamogordo', 'alan', 'alarm', 'alarmingli', 'alarmist', 'alaska',\n",
       "       'alastor', 'alba', 'albacor', 'albani', 'albania', 'albanian',\n",
       "       'albeit', 'alber', 'albert', 'alberto', 'albican', 'albright',\n",
       "       'album', 'albumin', 'alchemi', 'alcibiad', 'alcin', 'alcohol',\n",
       "       'alcorn', 'alcott'], dtype='<U20')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = brown.raw()\n",
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "corpus_size = len(words)\n",
    "print(corpus_size)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"number of tokens: \", len(tokens))\n",
    "words[500:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix2(text, words, word_to_index, window=1):\n",
    "    \"\"\" \n",
    "    Build a co-occurrence matrix \n",
    "    \n",
    "    Parameters: \n",
    "    text (string): A long string to be split into sentences.\n",
    "    words: A list of unique word names.\n",
    "    word_to_index: a mapping from word names to integers.\n",
    "    window: The size of the context window.\n",
    "  \n",
    "    Returns: \n",
    "    co_matrix: ndarray \n",
    "  \n",
    "    \"\"\"\n",
    "    porter = nltk.PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    corpus_size = len(words)\n",
    "    co_matrix = np.zeros((corpus_size,corpus_size),dtype=int)\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(porter.stem(w.lower()))\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                co_matrix[word_to_index[w],word_to_index[sent[j]]] += 1\n",
    "        np.fill_diagonal(co_matrix,0)\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix(text, window=1):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    counter = 0\n",
    "    co_matrix = pd.DataFrame();\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(w.lower())\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                if w == sent[j]:# skip the word itself\n",
    "                    co_matrix.loc[w,sent[j]] = 0\n",
    "                elif (w in co_matrix.index and sent[j] in co_matrix.columns) and not np.isnan(co_matrix.loc[w,sent[j]]):\n",
    "                    co_matrix.loc[w,sent[j]] += 1\n",
    "                else:\n",
    "                    co_matrix.loc[w,sent[j]] = 1\n",
    "    co_matrix.fillna(0, inplace=True)\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How tokenization with regex works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "data\n",
      "mining\n",
      "course\n",
      "cmp5101\n",
      "It\n",
      "is\n",
      "about\n",
      "data\n",
      "mining\n",
      "I\n",
      "like\n",
      "it\n",
      "so\n",
      "much\n"
     ]
    }
   ],
   "source": [
    "text = \"This is data mining course cmp5101. It is about data mining. I like it so much.\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for w in tokenizer.tokenize(text):  \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>data</th>\n",
       "      <th>mining</th>\n",
       "      <th>course</th>\n",
       "      <th>cmp5101</th>\n",
       "      <th>it</th>\n",
       "      <th>about</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "      <th>so</th>\n",
       "      <th>much</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mining</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmp5101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         this   is  data  mining  course  cmp5101   it  about    i  like   so  \\\n",
       "this      0.0  1.0   1.0     0.0     0.0      0.0  0.0    0.0  0.0   0.0  0.0   \n",
       "is        1.0  0.0   2.0     1.0     0.0      0.0  1.0    1.0  0.0   0.0  0.0   \n",
       "data      1.0  2.0   0.0     2.0     1.0      0.0  0.0    1.0  0.0   0.0  0.0   \n",
       "mining    0.0  1.0   2.0     0.0     1.0      1.0  0.0    1.0  0.0   0.0  0.0   \n",
       "course    0.0  0.0   1.0     1.0     0.0      1.0  0.0    0.0  0.0   0.0  0.0   \n",
       "cmp5101   0.0  0.0   0.0     1.0     1.0      0.0  0.0    0.0  0.0   0.0  0.0   \n",
       "it        0.0  1.0   0.0     0.0     0.0      0.0  0.0    1.0  1.0   1.0  1.0   \n",
       "about     0.0  1.0   1.0     1.0     0.0      0.0  1.0    0.0  0.0   0.0  0.0   \n",
       "i         0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   1.0  0.0   \n",
       "like      0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  1.0   0.0  1.0   \n",
       "so        0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   1.0  0.0   \n",
       "much      0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   0.0  1.0   \n",
       "\n",
       "         much  \n",
       "this      0.0  \n",
       "is        0.0  \n",
       "data      0.0  \n",
       "mining    0.0  \n",
       "course    0.0  \n",
       "cmp5101   0.0  \n",
       "it        1.0  \n",
       "about     0.0  \n",
       "i         0.0  \n",
       "like      0.0  \n",
       "so        1.0  \n",
       "much      0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = build_co_matrix(text, 2)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "matrix = build_co_matrix2(text, words, word_to_index, 2)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27114, 27114)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text = brown.raw()\n",
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "co_matrix = build_co_matrix2(text, words, word_to_index, 5)\n",
    "print(co_matrix.shape)\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity:\n",
    "Intuition: Dot product increases if both pairs have the same sign and decreases if pairs have different signs (similar to correlation, actually Pearson correlation is just cosine similarity of the mean centered vectors). Division by the norms is necessary to penalize vectors which has large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   1,   1,   1,   1,   1,   1,   1,   2,  29,   2,   1,   2,\n",
       "         3,   1,   2,  11,   2,   2,   1,   7,   1,   2,   1,   8,   1,\n",
       "         1,  17,   2,   1,   3,   1,   1,   1,   1,   1,   1,   6,   4,\n",
       "         1,   1,   1,   5,   7,   1,   9,   1,   1,   1,   1,   2,  10,\n",
       "         1,   3,   1,   2,   1,   1,  32,   1,  10,   1,   3,   3,   1,\n",
       "        10,   2,   1,   1,   1,   1,   3,   1,   1,   1,  21, 127,   1,\n",
       "         1,   1,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,   2,\n",
       "         1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   2,   1,   1,   1,   3,   1,   8,   1,   1,   3,   1,\n",
       "         1,   1,   1,   4,   1,   1,   1, 106,   1,   1,   1,   2,  10,\n",
       "        15,   1,   3,   1,   1,   1,   1,   3,   4,   1,   1,   7,   1,\n",
       "         4,   1,   3,   1,   4,   1,   1,   2,   1,   1,  14, 137,   1,\n",
       "         2,   1,   5,   1,  31,   3,   1,   9,   2,   1,   1,   1,   5,\n",
       "         1,   1,   1,   1,   4,   1,   1,   1,   4,   1,   1,   1,  14,\n",
       "         2,   1,   1,   2,   2,   2,   1,   1,   1,   1,   1,   1,   2,\n",
       "         1,   1,   1,   2,   1,   3,   2,  13,   2,   3,   1,   1,   1,\n",
       "         1,  34,   1,   2,   1,   1,   1,   1,   2,   6,   3,   6,  11,\n",
       "         3,   1,   1,   1,   1,   1,   1,   1,   4,   1,   2,   1,   1,\n",
       "         1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_matrix[:,0][np.nonzero(co_matrix[:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds cosine similarity between two vectors a and b\n",
    "def cosine(a, b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    return dot / (norma * normb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most similar words to the target word using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3621"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = word_to_index['book']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27114,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[target,:]\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27114, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = np.reshape(word_vector,(word_vector.size,1 ))\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117120,  31834, 476572, ...,   3606,    938,   2328])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = np.dot(word_vector.T,co_matrix)\n",
    "sims = sims[0,:]\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12229, 16505,  2343, 23958, 13061, 16875,  4753, 24294,  1783,\n",
       "       24296])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[12229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([231.17742104,  61.57921727, 809.71599959, ...,   7.07106781,\n",
       "         4.24264069,   6.        ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = np.linalg.norm(co_matrix, axis=0)\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.409673645990857"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sims = np.divide(sims,norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3621,  8957, 13715, 19713,  5609, 26775, 16887, 14957, 17524,\n",
       "       14157])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[3621]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'famili'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[8957]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3621,  8957, 13715, 19713,  5609, 26775, 16887, 14957, 17524,\n",
       "       14157])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[word_to_index['book'],:]\n",
    "word_vector = np.reshape(word_vector,(1,word_vector.size))\n",
    "sims = np.dot(word_vector,co_matrix)\n",
    "sims = sims[0,:]\n",
    "norm_sims = np.divide(sims,norms)\n",
    "top10 = norm_sims.argsort()[::-1][:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "famili\n",
      "land\n",
      "record\n",
      "command\n",
      "word\n",
      "offic\n",
      "mass\n",
      "paper\n",
      "line\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top10)):\n",
    "    print(index_to_word[top10[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12229, 16505,  2343, 23958, 13061, 16875,  4753, 24294,  1783,\n",
       "       24296])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[word_to_index['book'],:]\n",
    "word_vector = np.reshape(word_vector,(1,word_vector.size))\n",
    "sims = np.dot(word_vector,co_matrix)\n",
    "sims = sims[0,:]\n",
    "#norm_sims = np.divide(sims,norms)\n",
    "top10 = sims.argsort()[::-1][:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "nn\n",
      "at\n",
      "the\n",
      "jj\n",
      "of\n",
      "cc\n",
      "tl\n",
      "and\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top10)):\n",
    "    print(index_to_word[top10[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9818,  9022, 15911, 17579, 15530, 22291,  9110, 22466, 22349,\n",
       "       26037])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[word_to_index['friend'],:]\n",
    "word_vector = np.reshape(word_vector,(1,word_vector.size))\n",
    "sims = np.dot(word_vector,co_matrix)\n",
    "sims = sims[0,:]\n",
    "norm_sims = np.divide(sims,norms)\n",
    "top10 = norm_sims.argsort()[::-1][:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend\n",
      "father\n",
      "mother\n",
      "parent\n",
      "mine\n",
      "son\n",
      "fellow\n",
      "speech\n",
      "soul\n",
      "voic\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top10)):\n",
    "    print(index_to_word[top10[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
