{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Maximum Likelihood Estimation\n",
    "by Tevfik Aytekin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as f\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from random import choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Problem\n",
    "\n",
    "Suppose that we have a biased coin X. A biased coin means that the probability of a head is not 0.5. Suppose that we do not know the bias of X. In order to estimate its bias we toss the coin 10 times and the output is as follows:\n",
    "\n",
    "T, T, H, T, H, H, H, T, H, H\n",
    "\n",
    "What might be our best guess for the bias of X? \n",
    "\n",
    "You might think that since there are 4 Tails and 6 Heads in the sample output our best estimation for probabiliy of a head (let us call it $\\theta$) is 0.6. Actually, this reasoning can be made more formal as follows:\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "In order to understand MLE we need to first understand the likelihood function. Let $X$ be a discrete random variable whose whose values are H and T with probability $\\theta$ for H (hence 1-$\\theta$ for T) .\n",
    "\n",
    "Remember that in our problem we toss a coin 10 times and get 4 heads. What is the most probable value of $\\theta$ to give this outcome? For example, is $\\theta=0.1$ or $\\theta=0.5$ more probable? To find the answer let us formalize the problem.\n",
    "\n",
    "Let $X_1, X_2, X_3, ..., X_n$ denote a random sample where the probability mass (or density) function of each $X_i$ is $f(x_i;\\theta)$. Here, $x_i$ is an observed value of $X_i$ and $\\theta$ is some parameter (which can be more than one) of the pmf. Then the joint probability mass (or density) function of $X_1, X_2, X_3, ..., X_n$, which is called the likelihood function and denoted by $L(\\theta)$, is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta)&=P(X_1=x_1, X_2=x_2, X_3=x_3, ...,X_n=x_n) \\\\\n",
    "&= f(x_1;\\theta)f(x_2;\\theta)f(x_3;\\theta)...f(x_n;\\theta) \\\\ \n",
    "&= \\prod_{i=1}^{n}f(x_i;\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, let us find the likelihood function for our specific example. In our example the pmf of each $X_i$ can be described as a Bernoulli random variable with values 1 (for H) and 0 (for T) and with an unknown parameter $\\theta$ and whose pmf is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_i;\\theta) &= \\theta^{x_i}(1-\\theta)^{1-x_i}\\\\ \n",
    "\\implies L(\\theta)&=\\prod_{i=1}^{n}\\theta^{x_i}(1-\\theta)^{1-x_i}\\\\\n",
    "&= \\theta^{x_1}(1-\\theta)^{1-x_1}\\theta^{x_2}(1-\\theta)^{1-x_2}\\theta^{x_3}(1-\\theta)^{1-x_3}...\\theta^{x_n}(1-\\theta)^{1-x_n}\\\\\n",
    "&= \\theta^{\\sum x_i}(1-\\theta)^{n-\\sum x_i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Or simply, if there are $k$ heads in $n$ tosses, we can write the likelihood as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\theta^k(1-\\theta)^{(n-k)}\n",
    "$$ \n",
    "\n",
    "To find the most probable value of $\\theta$ means to find the value of $\\theta$ which maximizes the likelihood function (hence the name MLE). Now the rest is some calculus.\n",
    "\n",
    "For convinience we will take the log of $L$ (called the loglikelihood) since it will not change the maximizing value and it is easier to differentiate. \n",
    "\n",
    "$$\n",
    "logL(\\theta) =  klog\\theta + (n-k)log(1-\\theta)\n",
    "$$\n",
    "\n",
    "Now take the derivative with respect to $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial logL(\\theta)}{\\partial \\theta} =  \\frac{-k}{\\theta} + \\frac{(n-k)}{1-\\theta)}\n",
    "$$\n",
    "\n",
    "And solve by setting to zero\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{-k}{\\theta} + \\frac{(n-k)}{1-\\theta)} &= 0 \\\\\n",
    "\\implies -k(1-\\theta) + (n-k)\\theta &= 0 \\\\\n",
    "\\implies -k + \\theta k + \\theta n -\\theta k &= 0 \\\\\n",
    "\\implies \\theta &= \\frac{k}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, when $n=10$ and $k=4$, the most probably value of $\\theta=0.4$. This result conforms our initial intution. Note that in the above derivation we assumed that $L(\\theta)$ is a convex function.\n",
    "\n",
    "MLE is a very general technique and it can be applied to any parameter estimation problem given a sample output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
